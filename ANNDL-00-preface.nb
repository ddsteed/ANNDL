(* Content-type: application/vnd.wolfram.mathematica *)

(*** Wolfram Notebook File ***)
(* http://www.wolfram.com/nb *)

(* CreatedBy='Wolfram 14.1' *)

(*CacheID: 234*)
(* Internal cache information:
NotebookFileLineBreakTest
NotebookFileLineBreakTest
NotebookDataPosition[       154,          7]
NotebookDataLength[     25948,        518]
NotebookOptionsPosition[     22441,        443]
NotebookOutlinePosition[     22907,        460]
CellTagsIndexPosition[     22864,        457]
WindowFrame->Normal*)

(* Beginning of Notebook Content *)
Notebook[{
Cell[TextData[{
 StyleBox["Mohamed M. Hammad",
  FontFamily->"FZLanTingHei-DB-GBK",
  FontSize->12,
  FontWeight->"Regular",
  FontColor->RGBColor[
   0.8488288700694285, 0.3848325322346838, 0.1479972533760586]],
 StyleBox["\[LineSeparator]",
  FontSize->12,
  FontColor->RGBColor[1, 0.5, 0]],
 StyleBox["\n",
  FontSize->12,
  FontSlant->"Italic",
  FontColor->RGBColor[1, 0.5, 0]],
 StyleBox["Artificial Neural Network and Deep Learning                        \
            ",
  FontFamily->"Arial Unicode MS",
  FontSize->17.5,
  FontWeight->"Regular",
  FontColor->RGBColor[
   0.9866483558403907, 0.9388876173037308, 0.6336003662165255]],
 StyleBox[ButtonBox[">",
  BaseStyle->"Hyperlink",
  ButtonData->{
    FrontEnd`FileName[{$RootDirectory, "Users", "fengh", "Documents", "RDS", 
      "EDITED", "Artifical_Neural_Network_and_Deep_Learning"}, 
     "ANNDL-01-stat-prob.nb", CharacterEncoding -> "UTF-8"], None},
  ButtonNote->
   "/Users/fengh/Documents/RDS/EDITED/Artifical_Neural_Network_and_Deep_\
Learning/ANNDL-01-stat-prob.nb"],
  FontFamily->"Arial Unicode MS",
  FontSize->17.5,
  FontWeight->"Regular",
  FontColor->RGBColor[
   0.9866483558403907, 0.9388876173037308, 0.6336003662165255]],
 StyleBox["    ",
  FontFamily->"Arial Unicode MS",
  FontSize->17.5,
  FontWeight->"Regular",
  FontColor->RGBColor[
   0.9866483558403907, 0.9388876173037308, 0.6336003662165255]],
 StyleBox[ButtonBox["\[CapitalXi]",
  BaseStyle->"Hyperlink",
  ButtonData->{
    FrontEnd`FileName[{$RootDirectory, "Users", "fengh", "Documents", "RDS", 
      "EDITED", "Artifical_Neural_Network_and_Deep_Learning"}, "contents.nb", 
     CharacterEncoding -> "UTF-8"], None},
  ButtonNote->
   "/Users/fengh/Documents/RDS/EDITED/Artifical_Neural_Network_and_Deep_\
Learning/contents.nb"],
  FontFamily->"Arial Unicode MS",
  FontSize->17.5,
  FontWeight->"Regular",
  FontColor->RGBColor[
   0.9866483558403907, 0.9388876173037308, 0.6336003662165255]],
 StyleBox["\[LineSeparator]\[LineSeparator]",
  FontSize->12,
  FontSlant->"Italic",
  FontColor->RGBColor[1, 0.5, 0]],
 StyleBox["Edited by Hao Feng",
  FontFamily->"Futura",
  FontSize->12,
  FontWeight->"Medium",
  FontSlant->"Italic",
  FontColor->RGBColor[
   0.8488288700694285, 0.3848325322346838, 0.1479972533760586]]
}], "Text",
 CellMargins->{{66, -45}, {4, 12}},
 CellChangeTimes->{{3.9397640484222183`*^9, 3.939764052679113*^9}, 
   3.9397641564677134`*^9, 3.939764214184162*^9, {3.939774845841297*^9, 
   3.9397748486786137`*^9}, 3.9397769383984737`*^9, 3.939777524212697*^9, {
   3.939777679887363*^9, 3.9397776986053457`*^9}, 3.939777748637487*^9, {
   3.939777854556375*^9, 3.939777878635145*^9}, {3.9397779299447737`*^9, 
   3.9397779337440853`*^9}, {3.9397779656956463`*^9, 
   3.9397779872993917`*^9}, {3.939783995957651*^9, 3.9397839959644203`*^9}, 
   3.93994857128743*^9, {3.9403030753694696`*^9, 3.9403030753782463`*^9}, {
   3.940303217729404*^9, 3.940303217737211*^9}, {3.940741024123201*^9, 
   3.940741027365489*^9}, {3.940741081934002*^9, 3.940741081942062*^9}, {
   3.943567220607367*^9, 3.9435672414910088`*^9}, {3.94356735516547*^9, 
   3.943567355169693*^9}, {3.943575071816586*^9, 3.943575094124228*^9}, {
   3.943584562254546*^9, 3.943584562260049*^9}, {3.943587398111261*^9, 
   3.943587398115802*^9}},
 LineSpacing->{0.6999999999999997, 3},
 Background->RGBColor[
  0.13066300450141147`, 0.12460517280842298`, 0.4353551537346456],
 CellID->912160115,ExpressionUUID->"1c255bc0-c4e0-42e4-a82f-738775130196"],

Cell[CellGroupData[{

Cell["0 Frontmatter", "Section",
 CellChangeTimes->{
  3.652728456208679*^9, 3.652728527108994*^9, {3.943669519592314*^9, 
   3.943669523348545*^9}},
 CellID->1706356999,ExpressionUUID->"4d7ceec2-306f-4359-86de-daf2a2229642"],

Cell[CellGroupData[{

Cell["Abstract", "Subsection",
 CellChangeTimes->{
  3.652728456208679*^9, 3.652728527108994*^9, {3.943567532627124*^9, 
   3.94356753418813*^9}},
 CellID->267521681,ExpressionUUID->"72321a38-a4b0-4c7e-a0c8-2ac7257fba50"],

Cell["\<\
\[OpenCurlyDoubleQuote]Artificial Neural Network and Deep Learning: \
Fundamentals and Theory\[CloseCurlyDoubleQuote] offers a comprehensive \
exploration of the foundational principles and advanced methodologies in \
neural networks and deep learning. This book begins with essential concepts \
in descriptive statistics and probability theory, laying a solid groundwork \
for understanding data and probability distributions. As the reader \
progresses, they are introduced to matrix calculus and gradient optimization, \
crucial for training and fine-tuning neural networks. The book delves into \
multilayer feed-forward neural networks, explaining their architecture, \
training processes, and the backpropagation algorithm. Key challenges in \
neural network optimization, such as activation function saturation, \
vanishing and exploding gradients, and weight initialization, are thoroughly \
discussed. The text covers various learning rate schedules and adaptive \
algorithms, providing strategies to optimize the training process. Techniques \
for generalization and hyperparameter tuning, including Bayesian optimization \
and Gaussian processes, are also presented to enhance model performance and \
prevent overfitting. Advanced activation functions are explored in detail, \
categorized into sigmoid-based, ReLU- based, ELU-based, miscellaneous, \
non-standard, and combined types. Each activation function is examined for \
its properties and applications, offering readers a deep understanding of \
their impact on neural network behavior. The final chapter introduces \
complex-valued neural networks, discussing complex numbers, functions, and \
visualizations, as well as complex calculus and backpropagation algorithms. \
This chapter provides a comprehensive overview of complex activation \
functions and their unique properties. This book equips readers with the \
knowledge and skills necessary to design, and optimize advanced neural \
network models, contributing to the ongoing advancements in artificial \
intelligence.\
\>", "Text",
 CellChangeTimes->{{3.943567541767359*^9, 3.943567546317319*^9}, {
  3.943575107235924*^9, 3.943575111684477*^9}},
 CellID->938621000,ExpressionUUID->"06401c5a-6094-4f05-bd60-e893b0618742"]
}, Open  ]],

Cell[CellGroupData[{

Cell["Preface", "Subsection",
 CellChangeTimes->{
  3.652728456208679*^9, 3.652728527108994*^9, {3.93977430409951*^9, 
   3.939774306756702*^9}, {3.939774503882483*^9, 3.939774505098134*^9}},
 CounterAssignments->{{"Section", 0}},
 CellID->1815624021,ExpressionUUID->"56886e67-cbe3-4f27-ac08-c85ee17c234c"],

Cell["\<\
The rapid advancements in artificial intelligence over the past few decades \
have significantly transformed various industries, leading to breakthroughs \
that were once considered science fiction. Central to these advancements are \
artificial neural networks and deep learning, which have revolutionized \
fields such as computer vision, natural language processing, autonomous \
systems, and beyond. This book, \[OpenCurlyDoubleQuote]Artificial Neural \
Network and Deep Learning: Fundamentals and Theory,\[CloseCurlyDoubleQuote] \
aims to provide a comprehensive and in-depth exploration of these \
technologies, equipping readers with the theoretical knowledge needed to \
understand, and design advanced neural network models. \
\>", "Text",
 CellChangeTimes->{{3.943575150467389*^9, 3.943575169944619*^9}},
 CellID->789420508,ExpressionUUID->"e671359e-8983-4c75-bb85-8b3c3e196e9f"],

Cell["\<\
A central focus of this book is to introduce readers to the solid \
mathematical foundations underlying neural networks. This book is designed \
not only to cater to beginners in the field but also to serve as a valuable \
reference for seasoned data scientists, machine learning practitioners, \
biostatisticians, finance professionals, and engineers. Whether they possess \
prior knowledge of deep learning or seek to fill gaps in their understanding, \
this book aims to address their needs. We assume that the reader has no prior \
experience in neural networks and optimization. We tried to provide proofs as \
simply as possible so that any reader with a background in calculus could \
easily follow them. It can be used as a textbook for a course spanning one \
semester. \
\>", "Text",
 CellChangeTimes->{{3.943575150467389*^9, 3.943575178311472*^9}},
 CellID->1783924704,ExpressionUUID->"c82d9aec-1ceb-459c-909e-ab1a6af876e2"],

Cell["\<\
Each chapter is designed to build on the previous ones, creating a cohesive \
and comprehensive guide to artificial neural networks and deep learning. \
Whether you are a student, researcher, or practitioner, this book will equip \
you with the knowledge and skills to tackle the challenges of modern \
artificial intelligence and contribute to the ongoing advancements in this \
exciting field. \
\>", "Text",
 CellChangeTimes->{{3.943575150467389*^9, 3.943575186022274*^9}},
 CellID->1583982392,ExpressionUUID->"6e053b99-dab7-47e6-9fdc-ea9ae4a9ca28"],

Cell["\<\
The journey begins with Chapter 1: Descriptive Statistics and Probability \
Theory, where we lay the foundation by discussing essential concepts in \
statistics and probability. This chapter covers frequency distributions, \
histograms, and measures of central tendency and dispersion. It further \
delves into the concepts of symmetry and peakedness, random experiments, \
sample spaces, and counting techniques. The chapter concludes with a detailed \
examination of probability interpretations, axioms, conditional probability, \
and the different types of random variables and distributions, both discrete \
and continuous. Understanding these fundamental concepts is crucial for \
anyone looking to delve into machine learning and neural network algorithms, \
as they provide the statistical framework upon which these models are built. \
\
\>", "Text",
 CellChangeTimes->{{3.943575150467389*^9, 3.943575161936103*^9}, {
  3.943575193114493*^9, 3.9435751931387033`*^9}},
 CellID->1413944941,ExpressionUUID->"ca6a1a5d-aff4-467d-b9ba-6ab4c5f0413f"],

Cell["\<\
Moving forward, Chapter 2: Matrix Calculus and Gradient Optimization \
introduces the mathematical tools necessary for understanding and optimizing \
neural networks. This chapter covers vectors and matrices using both index \
and Dirac notations, basics of matrix calculus, and the chain rule. It also \
explores numerical differentiation, finite difference methods, and optimality \
criteria for functions of multiple variables. The concepts of gradient \
descent and other optimization techniques are thoroughly discussed, providing \
readers with the skills to optimize neural network models effectively. \
\>", "Text",
 CellChangeTimes->{{3.943575150467389*^9, 3.943575161936103*^9}, {
  3.943575198631351*^9, 3.943575198655917*^9}},
 CellID->1525448359,ExpressionUUID->"863f9ecc-2613-4f87-ab1a-9818d56df22c"],

Cell["\<\
Chapter 3: Multilayer Feed-Forward Neural Networks dives into the core \
architecture of neural networks. It explains feed-forward neural networks and \
forward propagation, presenting the multilayer network as a computational \
graph. The chapter also covers automatic differentiation, the training \
process, and loss/cost functions. It introduces the fundamental equations \
behind backpropagation and discusses various gradient descent algorithms, \
including batch, stochastic, and mini-batch methods. The chapter concludes \
with a discussion on linear activation functions and the universal \
approximation theorems, providing both intuitive and mathematical insights. \
\>", "Text",
 CellChangeTimes->{{3.943575150467389*^9, 3.943575161936103*^9}, {
  3.943575202547361*^9, 3.943575202572358*^9}},
 CellID->1705273201,ExpressionUUID->"5b83c005-24c4-48fb-a16a-b8a6448eeadc"],

Cell["\<\
In Chapter 4: Challenges in Neural Network Optimization, we address common \
issues faced when training neural networks. This chapter explores activation \
function saturation, vanishing and exploding gradients, and weight \
initialization techniques. It also discusses non-zero centered activation \
functions and feature scaling methods. Techniques such as normalization, \
standardization, and batch normalization are presented to help readers \
understand how to stabilize and improve the training process. \
\>", "Text",
 CellChangeTimes->{{3.943575150467389*^9, 3.943575161936103*^9}, {
  3.943575211424527*^9, 3.943575212455283*^9}},
 CellID->1013957291,ExpressionUUID->"ddca71e6-1ed3-4956-8197-6036fa6ebd5f"],

Cell["\<\
Chapter 5: Learning Rate Schedules and Adaptive Algorithms focuses on methods \
to adjust and optimize the learning rate during training. Various dynamic \
learning rate decay methods, including step decay, inverse time decay, and \
cyclical learning rates, are covered. The chapter also discusses accelerated \
gradient descent methods, adaptive learning rate algorithms, and \
Hessian-based methods such as Newton and Marquardt methods. Conjugate \
direction methods and quasi-Newton approaches are also presented, providing a \
comprehensive overview of advanced optimization techniques. \
\>", "Text",
 CellChangeTimes->{{3.943575150467389*^9, 3.943575161936103*^9}, {
  3.943575211424527*^9, 3.943575216232123*^9}},
 CellID->1675030702,ExpressionUUID->"79160c82-5a9d-4f9f-a4f1-15bc91849462"],

Cell["\<\
Chapter 6: Strategies for Generalization and Hyper-Parameter Tuning explores \
techniques to prevent overfitting and ensure that models generalize well to \
new data. This chapter covers statistical learning theory, point estimation, \
and the bias-variance trade-off. It also discusses training, testing, and \
validation sets, including cross-validation techniques. Performance measures \
for classification and regression models are presented, along with various \
hyperparameter tuning methods such as grid search, random search, and \
Bayesian optimization. The chapter concludes with a discussion on Gaussian \
processes and acquisition functions used in Bayesian optimization. \
\>", "Text",
 CellChangeTimes->{{3.943575150467389*^9, 3.943575161936103*^9}, {
  3.943575211424527*^9, 3.9435752197835417`*^9}},
 CellID->582575135,ExpressionUUID->"0c0fb409-913d-4e56-b406-52408e470c2e"],

Cell["\<\
Chapter 7: Regularization Techniques provides an in-depth look at methods to \
prevent overfitting and improve model performance. This chapter covers \
penalty-based regularization methods, including L2 (ridge) and L1 \
regularization, as well as elastic net regularization. Techniques such as \
early stopping, ensemble methods, and dropout are also discussed, providing \
readers with a toolkit of methods to improve model robustness. \
\>", "Text",
 CellChangeTimes->{{3.943575150467389*^9, 3.943575161936103*^9}, {
  3.943575211424527*^9, 3.9435752248538647`*^9}},
 CellID->1255182671,ExpressionUUID->"90162b32-078c-4c50-b670-e50a0ebdadf1"],

Cell["\<\
Chapter 8: Advanced Activation Functions delves into the diverse world of \
activation functions used in neural networks. This chapter categorizes \
activation functions into sigmoid-based, ReLU-based, ELU-based, \
miscellaneous, non-standard, and combined activation functions. Each type is \
discussed in detail, highlighting their properties, advantages, and \
applications. The chapter aims to provide a thorough understanding of the \
role of activation functions in neural network performance and their impact \
on model behavior. \
\>", "Text",
 CellChangeTimes->{{3.943575150467389*^9, 3.943575161936103*^9}, {
  3.943575211424527*^9, 3.943575227554418*^9}},
 CellID->1147996201,ExpressionUUID->"71881b78-7eef-4bf6-9aad-f2c4b3a35ca5"],

Cell["\<\
Chapter 9: Complex Valued Neural Networks introduces the concept of \
complex-valued neural networks and their unique properties. This chapter \
covers complex numbers, functions, and their visualizations, along with \
complex calculus and backpropagation algorithms. It discusses the properties \
and types of complex activation functions, providing a comprehensive overview \
of this advanced topic in neural network research. \
\>", "Text",
 CellChangeTimes->{{3.943575150467389*^9, 3.943575161936103*^9}, {
  3.943575211424527*^9, 3.943575232431629*^9}},
 CellID->720006440,ExpressionUUID->"5bcacbf1-3315-4481-9a7a-6ef88506c3e3"],

Cell["\<\
Finally, we extend our heartfelt thanks to Professor Mohamed Abdalla Darwish, \
Head of the Department of Mathematics and Computer Science, Faculty of \
Science, Damanhour University, Egypt, for his unwavering support. We are \
profoundly grateful to Professor Amr R. El Dhaba for his invaluable \
discussions and continued encouragement. \
\>", "Text",
 CellChangeTimes->{{3.943575150467389*^9, 3.943575161936103*^9}, {
  3.943575211424527*^9, 3.943575238569768*^9}},
 CellID->169004514,ExpressionUUID->"350318ec-8f9d-4ee8-bb4d-9293ea7bf0a2"],

Cell["\<\
We also wish to express our sincere appreciation to our colleagues and \
friends for their invaluable feedback, thoughtful comments, and constructive \
suggestions. In particular, we would like to acknowledge Professor Hamed \
Awad, Dr. Fatma El-Safty, Dr. Hamdy El Shamy, Dr. Mohamed Elhaddad, Mohamed \
Yahia, Ayman A. Abdelaziz, Eman Farag, Hassan M. Shetawy, Walaa Mansour, Moaz \
El-Essawey, Aziza Salah, and Eman R. Hendawy for their contributions. \
\>", "Text",
 CellChangeTimes->{{3.943575150467389*^9, 3.943575161936103*^9}, 
   3.943575211424527*^9, {3.9435752430731897`*^9, 3.94357524309774*^9}},
 CellID->544140152,ExpressionUUID->"3dbf25fc-1de6-41e7-8d0a-522b7d9d3b8f"],

Cell["\<\
We hope that you find this book informative and inspiring, and that it serves \
as a valuable resource in your journey through the world of artificial neural \
networks and deep learning.\
\>", "Text",
 CellChangeTimes->{{3.943575150467389*^9, 3.943575161936103*^9}, 
   3.943575211424527*^9},
 CellID->1870918578,ExpressionUUID->"19c1184e-68bc-45cb-8b54-3ac454ae6131"],

Cell["knowledge itself is power - Sir Francis Bacon 1597 ", "Text",
 CellChangeTimes->{{3.943567570061882*^9, 3.943567579071979*^9}, 
   3.943567728500908*^9, {3.9435677800777597`*^9, 3.943567780102065*^9}},
 TextAlignment->Right,
 FontSlant->"Italic",
 CellID->2126199977,ExpressionUUID->"bbf225c4-d799-47ed-8279-77d9579999d1"],

Cell["Egypt 2024 M. M. Hammad", "Text",
 CellChangeTimes->{{3.943567570061882*^9, 3.943567579071979*^9}, 
   3.943567728500908*^9},
 TextAlignment->Right,
 CellID->1443446675,ExpressionUUID->"5de77146-d576-4a91-b821-2de7821a7c46"]
}, Open  ]],

Cell[CellGroupData[{

Cell["Bridging Theory and Practice: The Dual Approach of This Book", \
"Subsection",
 CellChangeTimes->{3.652728456208679*^9, 3.652728527108994*^9, 
  3.9435678367517633`*^9},
 CellID->2001686121,ExpressionUUID->"1bd35bbd-8db8-4533-89fe-b833f97ce383"],

Cell["\<\
Many books on neural networks and deep learning tend to lean either heavily \
on theoretical aspects, with dense mathematical formulations, or focus \
predominantly on computational algorithms, often at the expense of a solid \
mathematical foundation. This book adopts a balanced approach that bridges \
the gap between these extremes. By seamlessly integrating theoretical \
principles with practical implementation, it empowers learners to fully \
harness the power of neural networks and deep learning in their pursuit of \
excellence across various domains. \
\>", "Text",
 CellChangeTimes->{{3.943567844516358*^9, 3.943567854557646*^9}},
 CellID->2007305392,ExpressionUUID->"6834510f-186c-4bb5-9932-347ede72882b"],

Cell["\<\
However, attempting to cover both the theoretical concepts and computational \
algorithms exhaustively within a single volume would be impractical. To \
ensure a thorough exploration of both aspects, this book is divided into two \
complementary parts. The first part is titled \
\[OpenCurlyDoubleQuote]Artificial Neural Network and Deep Learning: \
Fundamentals and Theory.\[CloseCurlyDoubleQuote] The second part is titled \
\[OpenCurlyDoubleQuote] Neural Network and Deep Learning with Mathematica\
\[CloseCurlyDoubleQuote] For each theoretical chapter in the first part, \
there is a corresponding chapter in the second part. We strongly recommend \
that after completing each theoretical chapter, you explore the corresponding \
practical implementation chapter in the complementary volume. This dual \
approach will provide you with a well-rounded understanding and the skills \
necessary to excel in this field. \
\>", "Text",
 CellChangeTimes->{{3.943567844516358*^9, 3.943567861678993*^9}},
 CellID->433903130,ExpressionUUID->"e417f1b7-a0d0-4a99-ac2e-914560edeed9"],

Cell["\<\
The book \[OpenCurlyDoubleQuote]Neural Network and Deep Learning with \
Mathematica\[CloseCurlyDoubleQuote] adopts a refreshingly code-centric \
approach, enabling you to solidify your understanding through hands-on \
practice. Nearly all the concepts introduced are accompanied by illustrative \
code examples, making the learning experience both practical and tangible. \
Even the figures in the first part are generated using these code examples, \
emphasizing the code-first methodology. To ensure accessibility and ease of \
understanding, the code examples are deliberately crafted in a simple format, \
prioritizing readability over efficiency and generality. In line with our \
instructional philosophy, each code example serves a dual purpose: not only \
does it demonstrate a specific deep learning concept, but it also \
simultaneously introduces and reinforces Mathematica programming techniques. \
Readers will learn how to leverage Mathematica to perform complex neural \
network and deep learning calculations, simulate data, and create visual \
representations of their findings.\
\>", "Text",
 CellChangeTimes->{{3.943567844516358*^9, 3.943567848742261*^9}},
 CellID->741060932,ExpressionUUID->"2e64733d-8885-465e-bd6a-6be4f65c7e49"]
}, Open  ]]
}, Open  ]]
},
WindowSize->{1024, 1099},
WindowMargins->{{Automatic, 0}, {Automatic, 0}},
FrontEndVersion->"14.1 for Mac OS X ARM (64-bit) (July 16, 2024)",
StyleDefinitions->FrontEnd`FileName[{"Wolfram"}, "BookToolsStyles.nb", 
  CharacterEncoding -> "UTF-8"],
ExpressionUUID->"8861bace-5584-420b-8e0c-c51fd7c6d3a1"
]
(* End of Notebook Content *)

(* Internal cache information *)
(*CellTagsOutline
CellTagsIndex->{}
*)
(*CellTagsIndex
CellTagsIndex->{}
*)
(*NotebookFileOutline
Notebook[{
Cell[554, 20, 3489, 85, 130, "Text",ExpressionUUID->"1c255bc0-c4e0-42e4-a82f-738775130196",
 CellID->912160115],
Cell[CellGroupData[{
Cell[4068, 109, 225, 4, 133, "Section",ExpressionUUID->"4d7ceec2-306f-4359-86de-daf2a2229642",
 CellID->1706356999],
Cell[CellGroupData[{
Cell[4318, 117, 221, 4, 68, "Subsection",ExpressionUUID->"72321a38-a4b0-4c7e-a0c8-2ac7257fba50",
 CellID->267521681],
Cell[4542, 123, 2261, 32, 724, "Text",ExpressionUUID->"06401c5a-6094-4f05-bd60-e893b0618742",
 CellID->938621000]
}, Open  ]],
Cell[CellGroupData[{
Cell[6840, 160, 306, 5, 68, "Subsection",ExpressionUUID->"56886e67-cbe3-4f27-ac08-c85ee17c234c",
 CounterAssignments->{{"Section", 0}},
 CellID->1815624021],
Cell[7149, 167, 899, 13, 281, "Text",ExpressionUUID->"e671359e-8983-4c75-bb85-8b3c3e196e9f",
 CellID->789420508],
Cell[8051, 182, 945, 14, 311, "Text",ExpressionUUID->"c82d9aec-1ceb-459c-909e-ab1a6af876e2",
 CellID->1783924704],
Cell[8999, 198, 562, 9, 163, "Text",ExpressionUUID->"6e053b99-dab7-47e6-9fdc-ea9ae4a9ca28",
 CellID->1583982392],
Cell[9564, 209, 1061, 16, 311, "Text",ExpressionUUID->"ca6a1a5d-aff4-467d-b9ba-6ab4c5f0413f",
 CellID->1413944941],
Cell[10628, 227, 824, 12, 252, "Text",ExpressionUUID->"863f9ecc-2613-4f87-ab1a-9818d56df22c",
 CellID->1525448359],
Cell[11455, 241, 891, 13, 281, "Text",ExpressionUUID->"5b83c005-24c4-48fb-a16a-b8a6448eeadc",
 CellID->1705273201],
Cell[12349, 256, 726, 11, 222, "Text",ExpressionUUID->"ddca71e6-1ed3-4956-8197-6036fa6ebd5f",
 CellID->1013957291],
Cell[13078, 269, 806, 12, 252, "Text",ExpressionUUID->"79160c82-5a9d-4f9f-a4f1-15bc91849462",
 CellID->1675030702],
Cell[13887, 283, 900, 13, 281, "Text",ExpressionUUID->"0c0fb409-913d-4e56-b406-52408e470c2e",
 CellID->582575135],
Cell[14790, 298, 654, 10, 193, "Text",ExpressionUUID->"90162b32-078c-4c50-b670-e50a0ebdadf1",
 CellID->1255182671],
Cell[15447, 310, 752, 12, 222, "Text",ExpressionUUID->"71881b78-7eef-4bf6-9aad-f2c4b3a35ca5",
 CellID->1147996201],
Cell[16202, 324, 643, 10, 193, "Text",ExpressionUUID->"5bcacbf1-3315-4481-9a7a-6ef88506c3e3",
 CellID->720006440],
Cell[16848, 336, 553, 9, 163, "Text",ExpressionUUID->"350318ec-8f9d-4ee8-bb4d-9293ea7bf0a2",
 CellID->169004514],
Cell[17404, 347, 693, 10, 193, "Text",ExpressionUUID->"3dbf25fc-1de6-41e7-8d0a-522b7d9d3b8f",
 CellID->544140152],
Cell[18100, 359, 379, 7, 104, "Text",ExpressionUUID->"19c1184e-68bc-45cb-8b54-3ac454ae6131",
 CellID->1870918578],
Cell[18482, 368, 328, 5, 45, "Text",ExpressionUUID->"bbf225c4-d799-47ed-8279-77d9579999d1",
 CellID->2126199977],
Cell[18813, 375, 230, 4, 45, "Text",ExpressionUUID->"5de77146-d576-4a91-b821-2de7821a7c46",
 CellID->1443446675]
}, Open  ]],
Cell[CellGroupData[{
Cell[19080, 384, 251, 4, 68, "Subsection",ExpressionUUID->"1bd35bbd-8db8-4533-89fe-b833f97ce383",
 CellID->2001686121],
Cell[19334, 390, 729, 11, 222, "Text",ExpressionUUID->"6834510f-186c-4bb5-9932-347ede72882b",
 CellID->2007305392],
Cell[20066, 403, 1084, 16, 340, "Text",ExpressionUUID->"e417f1b7-a0d0-4a99-ac2e-914560edeed9",
 CellID->433903130],
Cell[21153, 421, 1260, 18, 399, "Text",ExpressionUUID->"2e64733d-8885-465e-bd6a-6be4f65c7e49",
 CellID->741060932]
}, Open  ]]
}, Open  ]]
}
]
*)

